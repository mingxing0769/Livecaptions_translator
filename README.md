# livecpations_translator_to_sub
[中文说明]
本代码，使用Windows 11 自带的livecaptions程序识别语音，转录文本后用本地LLM模型翻译后输出到字幕

工作流程说明：

1.使用llama-cpp-python库加载量化版模型。

  个人建议使用qwen2.5，比较稳定，但翻译不够灵活，可能不太准确。
  
  使用deepseek-v2-lite模型翻译比较准确，但不稳定，会出现各种问题。
  
  其它模型测试不多。
  
2.启动windows内置的Livecaptions实时字幕程序，经过文本处理断句，只返回最后3句。

3.模型翻译：

  获取实时字幕程序返回的最后三句，进行处理。
  
  1.已经翻译的中 英文保存到历史信息中。
  
  2.对收到的英文句子（最后一句可能不完整，单独处理），如果没有存在于历史信息中，用模型翻译后保存到历史信息中
  
  3.最后一句单独翻译，不加入历史信息，当做即时信息处理。
  
  4.对最后一句单独的信息及最后三句翻译的历史信息，输出到字幕。

4.文本处理：

  1.英文按“.;?!”或使用from nltk import sent_tokenize 库进行断句，各有千秋，都不是完全准确，目前暂用sent_tokenize库进行断句
  
    注：标点以Livecaptions自动生成的，并没有后续添加。有时Livecaptions会生成很长都没有标点的句子，会增加模型翻译的延时。可以和断句模型处理，但不稳定，所以暂时没加上。
    
  2.中文按“。；？！”进行断句，仅用时字幕显示，最后三句，避免显示文本太多。

5.对话历史维护。

  模型的对话历史，设置为小于模型设置的n_ctx * 75% total_tokens大于n_ctx*0.75时 仅保留最后3组对话及系统指令。

流程循环为：1.Livecaptions文本 --> 2.文本处理得到最后三句 --> 3.模型翻译（判断输入，翻译历史信息存储、对话信息处理，字幕文本处理) --> 4.输出到字幕程序 --> 1.

总体延时，正常情况下(Livecaptions断句合理),关键在翻译模型处理时间，目前最少设置为1秒，如果本地GPU处理能力强，可以看情况设置，但意义不大，因为0.5秒内可能就增加一两个单词，只会增加电费。

[EN]:

This code utilizes Windows 11's built-in LiveCaptions program to recognize speech, transcribe the text, and then translates it using a local LLM model before outputting it as subtitles.

Workflow Description:
Model Loading

The llama-cpp-python library is used to load a quantized model.

Recommended Models:

Qwen2.5: More stable but may lack flexibility in translation, leading to inaccuracies.

Deepseek-v2-lite: More accurate in translation but less stable, potentially causing various issues.

Other models have not been extensively tested.

Live Captions Initialization

The built-in Windows Live Captions program is launched.

After text processing and sentence segmentation, only the last three sentences are returned.

Model Translation Process

The last three sentences from Live Captions are processed as follows:

History Storage: Translated Chinese and English sentences are saved in history.

New Sentence Handling: If an English sentence (the last one may be incomplete) is not in the history, it is translated and stored.

Last Sentence Handling: The final sentence is translated separately and treated as real-time information (not added to history).

Output: The last translated sentence (real-time) and the last three translated sentences (historical) are displayed as subtitles.

Text Processing

English Segmentation:

Sentences are split using .;?! or the nltk.sent_tokenize library.

Note: Punctuation is generated by Live Captions without post-processing. Long sentences without punctuation may increase translation delay. Alternative segmentation methods exist but are unstable, so they are not currently implemented.

Chinese Segmentation:

Sentences are split using 。；？！ for subtitle display, showing only the last three sentences to avoid clutter.

Dialogue History Management

The model’s dialogue history is limited to less than 75% of n_ctx tokens.

If the total tokens exceed 75% of n_ctx, only the last three dialogue exchanges and system instructions are retained.

Process Loop:
Live Captions Text → 2. Text Processing (Last 3 Sentences) → 3. Model Translation (Input Check, History Storage, Dialogue Handling, Subtitle Processing) → 4. Output to Subtitle Program → (Repeat)

Latency:
Under normal conditions (proper Live Captions segmentation), the primary delay comes from the translation model.

The minimum processing time is currently set to 1 second.

With a powerful local GPU, this can be adjusted, but reducing it further (e.g., below 0.5 seconds) may only add minor words without significant benefit, increasing power consumption unnecessarily.
